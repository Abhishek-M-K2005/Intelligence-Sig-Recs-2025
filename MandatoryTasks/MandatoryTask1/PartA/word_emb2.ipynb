{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cc402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading sqlalchemy-1.4.54.tar.gz (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting Mako (from alembic>=0.6.2->dataset)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/abhishekmarutikarennavar/Library/Python/3.9/lib/python/site-packages (from alembic>=0.6.2->dataset) (4.15.0)\n",
      "Requirement already satisfied: tomli in /Users/abhishekmarutikarennavar/Library/Python/3.9/lib/python/site-packages (from alembic>=0.6.2->dataset) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/abhishekmarutikarennavar/Library/Python/3.9/lib/python/site-packages (from Mako->alembic>=0.6.2->dataset) (3.0.2)\n",
      "Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
      "Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: sqlalchemy\n",
      "\u001b[33m  DEPRECATION: Building 'sqlalchemy' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sqlalchemy'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.54-cp39-cp39-macosx_10_9_universal2.whl size=1588049 sha256=f70450823f4fa5fc4117e0d1c9479739c7daf59ab11ce24a8c8e26711ef6ddb1\n",
      "  Stored in directory: /Users/abhishekmarutikarennavar/Library/Caches/pip/wheels/76/d6/51/e82ea8caaec4f0dd1756493d8ecec6a75f49d152f7d2ede4e1\n",
      "Successfully built sqlalchemy\n",
      "Installing collected packages: banal, sqlalchemy, Mako, alembic, dataset\n",
      "\u001b[2K  Attempting uninstall: sqlalchemy\n",
      "\u001b[2K    Found existing installation: SQLAlchemy 2.0.37\n",
      "\u001b[2K    Uninstalling SQLAlchemy-2.0.37:\n",
      "\u001b[2K      Successfully uninstalled SQLAlchemy-2.0.37\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [dataset]m3/5\u001b[0m [alembic]my]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask-sqlalchemy 3.1.1 requires sqlalchemy>=2.0.16, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.10 alembic-1.16.5 banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.54\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2529008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tf_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Embedding, GlobalAveragePooling1D, \n",
    "                                     LSTM, Bidirectional, Dropout, Input, Concatenate)\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"lucadiliello/newsqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405b935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset, max_samples=50000):\n",
    "    \"\"\"Extract contexts and questions from NewsQA dataset\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for split in ['train', 'validation']:\n",
    "        for item in dataset[split]:\n",
    "            if item['context']:\n",
    "                texts.append(item['context'])\n",
    "            if item['question']:\n",
    "                texts.append(item['question'])\n",
    "            \n",
    "            if len(texts) >= max_samples:\n",
    "                break\n",
    "        if len(texts) >= max_samples:\n",
    "            break\n",
    "    \n",
    "    return texts[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc558d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0fb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7be44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = prepare_data(ds, max_samples=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting vectorizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Adapting vectorizer...\")\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(texts)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "vocab = vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f453864",
   "metadata": {},
   "source": [
    "# LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486df2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSequencesLstm(texts, vectorize_layer, seq_length=50):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for text in texts[:10000]:\n",
    "        vectorized = vectorize_layer([text]).numpy()[0]\n",
    "        for i in range(1, len(vectorized)):\n",
    "            if vectorized[i] == 0:\n",
    "                break\n",
    "            X.append(vectorized[:i])\n",
    "            y.append(vectorized[i])\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=seq_length, padding='pre')\n",
    "    return np.array(X_padded), np.array(y)\n",
    "\n",
    "print(\"Creating LSTM training sequences...\")\n",
    "X_lstm, y_lstm = createSequencesLstm(texts, vectorize_layer)\n",
    "print(f\"Created {len(X_lstm)} sequences\")\n",
    "\n",
    "# Build LSTM model\n",
    "embedding_dim_2 = 128\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim_2, name='lstm_embedding'),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model.fit(X_lstm, y_lstm, epochs=5, batch_size=256, validation_split=0.1, verbose=1)\n",
    "\n",
    "\n",
    "lstm_embeddings = lstm_model.get_layer('lstm_embedding').get_weights()[0]\n",
    "save_embeddings_to_csv(lstm_embeddings, vocab, 'lstm_contextual_embeddings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db201d",
   "metadata": {},
   "source": [
    "# Multi-task embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_4 = 128\n",
    "\n",
    "input_layer = Input(shape=(sequence_length,), name='input')\n",
    "embedding = Embedding(vocab_size, embedding_dim_4, name='multitask_embedding')(input_layer)\n",
    "\n",
    "# Task 1: Next word prediction\n",
    "lstm_out = LSTM(64, return_sequences=True)(embedding)\n",
    "next_word = GlobalAveragePooling1D()(lstm_out)\n",
    "next_word_pred = Dense(vocab_size, activation='softmax', name='next_word')(next_word)\n",
    "\n",
    "# Task 2: Sentence classification (based on length/complexity)\n",
    "pooled = GlobalAveragePooling1D()(embedding)\n",
    "classification = Dense(64, activation='relu')(pooled)\n",
    "class_output = Dense(3, activation='softmax', name='classification')(classification)\n",
    "\n",
    "multitask_model = Model(inputs=input_layer, outputs=[next_word_pred, class_output])\n",
    "multitask_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'next_word': 'sparse_categorical_crossentropy', 'classification': 'sparse_categorical_crossentropy'},\n",
    "    loss_weights={'next_word': 1.0, 'classification': 0.5},\n",
    "    metrics={'next_word': 'accuracy', 'classification': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Prepare multi-task data\n",
    "X_multi = X_lstm[:5000]\n",
    "y_next_word = y_lstm[:5000]\n",
    "y_class = np.random.randint(0, 3, size=(len(X_multi),))  # Dummy classification labels\n",
    "\n",
    "print(\"Training multi-task model...\")\n",
    "multitask_model.fit(X_multi, {'next_word': y_next_word, 'classification': y_class}, \n",
    "                   epochs=5, batch_size=256, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Extract embeddings\n",
    "multitask_embeddings = multitask_model.get_layer('multitask_embedding').get_weights()[0]\n",
    "save_embeddings_to_csv(multitask_embeddings, vocab, 'multitask_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01a427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
